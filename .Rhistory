library(stats) # preinstalled with R generally
library(FactoMineR)
library(factoextra)
library(cluster)
library(mclust)
library(mlbench)
library(randomForest)
library(rpart)
library(entropy)
library(pROC)
library(ggplot2)
library(cluster)
attach(Assignment_section_3_data_set)
library(readxl)
Assignment_section_3_data_set <- read_excel("Data/Assignment section 3 data set.xlsx")
View(Assignment_section_3_data_set)
library(stats) # preinstalled with R generally
library(FactoMineR)
library(factoextra)
library(cluster)
library(mclust)
library(mlbench)
library(randomForest)
library(rpart)
library(entropy)
library(pROC)
library(ggplot2)
library(cluster)
attach(Assignment_section_3_data_set)
data <- Assignment_section_3_data_set[-1]
#optional
library(randomForestSRC)
## hopefully this too
library(entropy)
train <- subset(data, set == 'train')
test <- subset(data, set == 'test')
mc = Mclust(data)
dr = MclustDR(object = mc, lambda = 1, normalized = T)
plot(dr) # check options 1, 5 for sure
exit
0
plot(dr, legend = TRUE) # check options 1, 5 for sure
plot(dr, legend(TRUE)) # check options 1, 5 for sure
# Create a color palette for the clusters
colors <- rainbow(n = length(unique(mc$classification)))
# Plot the scatterplot with cluster labels as colors
plot(dr, type = "scatterplot", col = colors[mc$classification], main = "MclustDR Scatterplot", xlab = "Dimension 1", ylab = "Dimension 2", legend = TRUE)
# Add a legend to the plot
legend("topleft", legend = levels(as.factor(mc$classification)), fill = colors, title = "Cluster Labels")
#Create a color palette for the clusters
colors <- rainbow(n = length(unique(mc$classification)))
# Plot the scatterplot with cluster labels as colors
plot(dr, type = "scatterplot", col = colors[mc$classification], main = "MclustDR Scatterplot", xlab = "Dimension 1", ylab = "Dimension 2", legend = TRUE)
# Get the variable names from the original data
variable_names <- names(data)[-1]  # exclude the first column which is assumed to be the ID or label column
# Add a legend to the plot with variable names
legend("topleft", legend = paste(variable_names, levels(as.factor(mc$classification)), sep = " - "), fill = colors, title = "Cluster Labels")
#building the tree
model = rpart(as.factor(diagnosis) ~ ., data = (train), method = 'class')
par(xpd = T)
plot(model)
text(model, use.n = TRUE)
table(as.factor(train$diagnosis), predict(object = model, newdata = train, type = 'class'))
summary(model)
library(caret)
confusionMatrix(model)
rf = randomForest(as.factor(diagnosis) ~ ., data = (train),nodesize = 20, ntree = 10, do.trace = F)
rf
print(rf)
table(as.factor(test$diagnosis), predict(object = model, newdata = test, type = 'class'))
View(test)
View(train)
summary(model)
summary(train$diagnosis)
table(train$diagnosis)
table(test$diagnosis)
levels(train$diagnosis)
levels(test$diagnosis)
levels(as.factor(train$diagnosis))
levels(as.factor(test$diagnosis))
sum(is.na(test))
table(as.factor(test$diagnosis), predict(object = model, newdata = test, type = 'class'))
summary(model)
test$diagnosis <- factor(test$diagnosis, levels = levels(train$diagnosis))
table(as.factor(test$diagnosis), predict(object = model, newdata = test, type = 'class'))
table(as.factor(test$diagnosis), predict(object = model, newdata = test, type = 'class', drop.unused.levels = TRUE))
test <- subset(data, set == 'test')
table(as.factor(test$diagnosis), predict(object = model, newdata = test, type = 'class', drop.unused.levels = TRUE))
table(as.factor(test$diagnosis), predict(object = model, newdata = test[-1], type = 'class'))
#building the tree
model = rpart(as.factor(diagnosis) ~ ., data = (train[-1]), method = 'class')
table(as.factor(test[-1]$diagnosis), predict(object = model, newdata = test[-1], type = 'class'))
rf = randomForest(as.factor(diagnosis) ~ ., data = (train), ntree = 10, do.trace = F)
table(as.factor(diagnosis), predict(object = rf, newdata = test[-1], type = 'class'))
rf = randomForest(as.factor(diagnosis) ~ ., data = (train[-1]), ntree = 10, do.trace = F)
table(as.factor(diagnosis), predict(object = rf, newdata = test[-1], type = 'class'))
rf = randomForest(as.factor(diagnosis) ~ ., data = (train), nodesize = 25, ntree = 10)
table(as.factor(test$tdiagnosis), predict(object = rf, newdata = test, type = 'class'))
table(as.factor(test$diagnosis), predict(object = rf, newdata = test, type = 'class'))
rf = randomForest(as.factor(diagnosis) ~ ., data = (train), nodesize = 25, ntree = 50)
table(as.factor(test$diagnosis), predict(object = rf, newdata = test, type = 'class'))
rf = randomForest(as.factor(diagnosis) ~ ., data = (train), nodesize = 25, ntree = 500)
table(as.factor(test$diagnosis), predict(object = rf, newdata = test, type = 'class'))
rf = randomForest(as.factor(diagnosis) ~ ., data = (train), ntree = 500)
table(as.factor(test$diagnosis), predict(object = rf, newdata = test, type = 'class'))
rf = randomForest(as.factor(diagnosis) ~ ., data = (train), ntree = 5000)
table(as.factor(test$diagnosis), predict(object = rf, newdata = test, type = 'class'))
45/(45+8)
82/(82+70
82/(82+70)
82/(82+70)
82/(82+7)
83/(83+6)
47/(47+6)
prp(model)
library(rpart.plot)
prp(model)
summary(model)
View(data)
#entropy
repEn <- function(k) {
t1 = sum(table(data$diagnosis[data$diagnosis > k]))
t2 = sum(table(data$diagnosis[data$diagnosis <= k]))
en1 = entropy(table(data$diagnosis[data$diagnosis > k]), unit = 'log2')
en2 = entropy(table(data$diagnosis[data$diagnosis <= k]), unit = 'log2')
return((en1*t1/(t1+t2)) + (en2*t2/(t1+t2)))
}
repEn(rf)
repEn(106)
warnings()
repEn(0.1584)
repEn(0.1013)
en2 = entropy(table(data$diagnosis[data$perimeter_worst <= k]), unit = 'log2')
#entropy
repEn <- function(k) {
t1 = sum(table(data$diagnosis[data$perimeter_worst > k]))
t2 = sum(table(data$diagnosis[data$perimeter_worst <= k]))
en1 = entropy(table(data$diagnosis[data$perimeter_worst > k]), unit = 'log2')
en2 = entropy(table(data$diagnosis[data$perimeter_worst <= k]), unit = 'log2')
return((en1*t1/(t1+t2)) + (en2*t2/(t1+t2)))
}
repEn(106)
#entropy
repEn <- function(k) {
t1 = sum(table(data$diagnosis[data$concave.points_worst > k]))
t2 = sum(table(data$diagnosis[data$concave.points_worst <= k]))
en1 = entropy(table(data$diagnosis[data$concave.points_worst > k]), unit = 'log2')
en2 = entropy(table(data$diagnosis[data$concave.points_worst <= k]), unit = 'log2')
return((en1*t1/(t1+t2)) + (en2*t2/(t1+t2)))
}
repEn(0.1584)
#entropy
repEn <- function(k) {
t1 = sum(table(data$diagnosis[data$concave.points_mean > k]))
t2 = sum(table(data$diagnosis[data$concave.points_mean <= k]))
en1 = entropy(table(data$diagnosis[data$concave.points_mean > k]), unit = 'log2')
en2 = entropy(table(data$diagnosis[data$concave.points_mean <= k]), unit = 'log2')
return((en1*t1/(t1+t2)) + (en2*t2/(t1+t2)))
}
repEn(0.04952)
#entropy
repEn <- function(k) {
t1 = sum(table(data$diagnosis[data$concave.points_se > k]))
t2 = sum(table(data$diagnosis[data$concave.points_se <= k]))
en1 = entropy(table(data$diagnosis[data$concave.points_se > k]), unit = 'log2')
en2 = entropy(table(data$diagnosis[data$concave.points_se <= k]), unit = 'log2')
return((en1*t1/(t1+t2)) + (en2*t2/(t1+t2)))
}
repEn(0.01013)
summary(rf)
View(rf)
rf[["importance"]]
rf = randomForest(as.factor(diagnosis) ~ ., data = (train), ntree = 500)
rf[["confusion"]]
table(as.factor(test$diagnosis), predict(object = rf, newdata = test, type = 'class'))
View(rf)
rf[["importance"]]
rf[["oob.times"]]
rf[["localImportance"]]
rf[["forest"]]
rf = randomForest(as.factor(diagnosis) ~ ., data = (train), ntree = 10)
View(rf)
imp <- importance(rf)
imp
imp_rf <- importance(rf)
imp_rf
imp_mod <- importance(model)
#building the tree
model = rpart(as.factor(diagnosis) ~ ., data = (train[-1]), method = 'class')
imp_mod <- importance(model)
imp_mod <- varImp(model)
imp_mod
?varImp
imp_mod <- importance(model, type = 2)
summary(model)
imp_mod <- importance(model, type = 1)
imp_mod
imp_mod <- importance(model)
imp_mod <- ivarImp(model)
imp_mod <- varImp(model)
imp_mod
imp_rf <- importance(rf)
imp_rf
imp_rf <- importance(rf)
imp_rf_sorted <- sort(imp_rf, decreasing = TRUE)
imp_rf_sorted
imp_rf <- importance(rf)
sorted_imp_rf <- sort(imp_rf, decreasing = TRUE)
names(sorted_imp_rf) <- names(imp_rf)
sorted_imp_rf
plot(dr) # check options 1, 5 for sure
View(Assignment_section_3_data_set)
View(data)
test$diagnosis <- factor(test$diagnosis, levels = levels(train$diagnosis))
#entropy
repEn <- function(k) {
t1 = sum(table(data$diagnosis[data$concave.points_se > k]))
t2 = sum(table(data$diagnosis[data$concave.points_se <= k]))
en1 = entropy(table(data$diagnosis[data$concave.points_se > k]), unit = 'log2')
en2 = entropy(table(data$diagnosis[data$concave.points_se <= k]), unit = 'log2')
return((en1*t1/(t1+t2)) + (en2*t2/(t1+t2)))
}
repEn(0.01013)
imp_rf <- importance(rf)
sorted_imp_rf <- sort(imp_rf, decreasing = TRUE)
names(sorted_imp_rf) <- names(imp_rf)
sorted_imp_rf
mc1 = Mclust(test[-1])
test <- subset(data, set == 'test')
mc1 = Mclust(test[-1])
dr1 = MclustDR(object = mc1, lambda = 1, normalized = T)
plot(dr1) # check options 1, 5 for sure
plot(dr) # check options 1, 5 for sure
plot(dr, main = "MclustDR Plot for Data", legend = TRUE) # check options 1, 5 for sure
imp_rf
